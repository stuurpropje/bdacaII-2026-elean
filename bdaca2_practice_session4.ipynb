{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDACA II: Practice Session – From Classifier to Publishable Workflow\n",
    "\n",
    "In this session, you'll put the lecture concepts into practice:\n",
    "\n",
    "- Proper data splitting (and understanding when to use what)\n",
    "- Building pipelines\n",
    "- Hyperparameter tuning with cross-validation\n",
    "- Understanding regularization\n",
    "- Thorough evaluation\n",
    "- Saving and reusing models\n",
    "\n",
    "You have two files available:\n",
    "\n",
    "- `labeled.csv` — your annotated training/evaluation data\n",
    "- `unlabeled.csv` — data you might eventually want to classify\n",
    "\n",
    "**Goal:** Build a robust, well-validated text classifier that you could defend\n",
    "in a thesis or paper.\n",
    "\n",
    "**Book reference:** This session builds on\n",
    "[Chapter 11.4 of the CSS book](https://cssbook.net/content/chapter11.html#sec-supervised)\n",
    "— you may want to keep it open for reference.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib  # for saving models\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    StratifiedKFold, \n",
    "    cross_val_score, \n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV  # alternative to GridSearchCV\n",
    ")\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For reproducibility - ALWAYS set this!\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore the Data\n",
    "\n",
    "Before doing anything else: understand your data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'labeled.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlabeled.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m documents\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m df.head()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Elean\\.conda\\envs\\bigdata2\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Elean\\.conda\\envs\\bigdata2\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Elean\\.conda\\envs\\bigdata2\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Elean\\.conda\\envs\\bigdata2\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Elean\\.conda\\envs\\bigdata2\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'labeled.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"labeled.csv\")\n",
    "print(f\"Dataset size: {len(df)} documents\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Examine the label distribution\n",
    "# This is crucial - is it balanced? Imbalanced?\n",
    "# Hint: use .value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question to consider:** Based on the label distribution, what should you keep\n",
    "in mind when:\n",
    "\n",
    "1. Splitting your data?\n",
    "2. Choosing evaluation metrics?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Splitting Strategies\n",
    "\n",
    "There are two main approaches (as covered in the lecture):\n",
    "\n",
    "### Option A: Train / Validation / Test Split\n",
    "\n",
    "- **Train** (e.g., 64%): fit models\n",
    "- **Validation** (e.g., 16%): compare models, tune hyperparameters\n",
    "- **Test** (e.g., 20%): final evaluation, touched only once\n",
    "\n",
    "**When to use:** Very large datasets, computationally expensive models (e.g.,\n",
    "deep learning)\n",
    "\n",
    "### Option B: Train / Test Split + Cross-Validation\n",
    "\n",
    "- **Train** (e.g., 80%): used for k-fold CV during tuning\n",
    "- **Test** (e.g., 20%): final evaluation, touched only once\n",
    "\n",
    "**When to use:** Smaller datasets where you want stable estimates (our case\n",
    "today)\n",
    "\n",
    "**Book reference:** See the\n",
    "[CSS book section 11.4.1](https://cssbook.net/content/chapter11.html#sec-workflow)\n",
    "for more on workflows.\n",
    "\n",
    "---\n",
    "\n",
    "We'll use **Option B** today. First, let's create our train/test split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a train/test split\n",
    "# Requirements:\n",
    "# - 80% train, 20% test\n",
    "# - Use stratification (why? → preserves label distribution in both sets)\n",
    "# - Set the random state for reproducibility\n",
    "\n",
    "# Hint: check the parameters of train_test_split()\n",
    "\n",
    "X = df['text']  # adjust column name if needed\n",
    "y = df['label']  # adjust column name if needed\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify your split preserved the label distribution\n",
    "print(\"Original distribution:\")\n",
    "print(y.value_counts(normalize=True).round(3))\n",
    "print(\"\\nTraining distribution:\")\n",
    "print(y_train.value_counts(normalize=True).round(3))\n",
    "print(\"\\nTest distribution:\")\n",
    "print(y_test.value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) If you wanted a three-way split instead:\n",
    "\n",
    "```python\n",
    "# First split: separate test set\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Second split: separate train and validation from the remainder\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.2, stratify=y_temp, random_state=RANDOM_STATE\n",
    ")  # 0.2 of 0.8 = 0.16 of total\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a Pipeline\n",
    "\n",
    "Pipelines bundle preprocessing and classification into one object. This is\n",
    "important because:\n",
    "\n",
    "1. **Prevents data leakage** during cross-validation (vectorizer is fit only on\n",
    "   training folds)\n",
    "2. Makes hyperparameter tuning cleaner\n",
    "3. Easier to save and deploy\n",
    "\n",
    "**Book reference:** See\n",
    "[CSS book Example 11.5](https://cssbook.net/content/chapter11.html#exm-basicpipe)\n",
    "for pipeline basics.\n",
    "\n",
    "Let's start with a simple baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple baseline pipeline\n",
    "baseline_pipe = Pipeline([\n",
    "    (\"vectorizer\", TfidfVectorizer()),\n",
    "    (\"classifier\", LogisticRegression(solver=\"liblinear\", random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "# Quick sanity check - does it run?\n",
    "baseline_pipe.fit(X_train, y_train)\n",
    "print(f\"Baseline accuracy on training data: {baseline_pipe.score(X_train, y_train):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Why shouldn't we report that training accuracy as our result?\n",
    "(Think: overfitting)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross-Validation\n",
    "\n",
    "Instead of a single train/validation split, we use k-fold cross-validation to\n",
    "get a more stable estimate of performance.\n",
    "\n",
    "**Remember:** We're still only using `X_train` and `y_train` here. The test set\n",
    "stays untouched!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up stratified k-fold cross-validation\n",
    "# - Stratified: preserves label distribution in each fold\n",
    "# - shuffle=True: randomize before splitting\n",
    "# - random_state: for reproducibility\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use cross_val_score to evaluate the baseline pipeline\n",
    "# Try different scoring metrics: 'accuracy', 'f1_macro', 'f1_weighted'\n",
    "#\n",
    "# Which one(s) should you report given your label distribution?\n",
    "# - Balanced classes → accuracy is fine\n",
    "# - Imbalanced classes → F1 (especially macro) is more informative\n",
    "\n",
    "# scores = cross_val_score(baseline_pipe, X_train, y_train, cv=cv, scoring=\"f1_macro\")\n",
    "# print(f\"CV F1 (macro): {scores.mean():.3f} (+/- {scores.std():.3f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Understanding Regularization\n",
    "\n",
    "Before we tune hyperparameters, let's understand what we're tuning.\n",
    "\n",
    "### The Problem: High-Dimensional Text\n",
    "\n",
    "Text classification has **many features** (words) and **relatively few\n",
    "documents**. Without constraints, models will overfit by assigning large weights\n",
    "to rare words that happen to correlate with labels in the training data.\n",
    "\n",
    "### The Solution: Regularization\n",
    "\n",
    "Regularization adds a penalty for large coefficients, forcing the model to be\n",
    "simpler.\n",
    "\n",
    "| Type            | Penalty                      | Effect                                | When to use                                        |\n",
    "| --------------- | ---------------------------- | ------------------------------------- | -------------------------------------------------- |\n",
    "| **L2 (Ridge)**  | Sum of squared coefficients  | Many small coefficients               | Default for text, good all-rounder                 |\n",
    "| **L1 (Lasso)**  | Sum of absolute coefficients | Some coefficients become exactly zero | When you want feature selection / interpretability |\n",
    "| **Elastic Net** | Mix of L1 and L2             | Compromise                            | When you want some sparsity but L1 is unstable     |\n",
    "\n",
    "### The C Parameter\n",
    "\n",
    "In scikit-learn, `C` controls regularization strength:\n",
    "\n",
    "- **Small C** (e.g., 0.01) → Strong regularization → Simpler model\n",
    "- **Large C** (e.g., 100) → Weak regularization → Model can fit training data\n",
    "  more closely (risk of overfitting)\n",
    "\n",
    "Note: C is the _inverse_ of regularization strength (λ in the lecture slides),\n",
    "which can be confusing!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick demonstration: effect of C on number of \"active\" features with L1\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec = TfidfVectorizer(min_df=5, max_df=0.5)\n",
    "X_train_vec = vec.fit_transform(X_train)\n",
    "\n",
    "for C in [0.01, 0.1, 1, 10]:\n",
    "    lr = LogisticRegression(C=C, penalty='l1', solver='liblinear', random_state=RANDOM_STATE)\n",
    "    lr.fit(X_train_vec, y_train)\n",
    "    n_nonzero = (lr.coef_ != 0).sum()\n",
    "    print(f\"C={C:5}: {n_nonzero:4} non-zero coefficients out of {lr.coef_.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Hyperparameter Tuning with GridSearchCV\n",
    "\n",
    "Now let's systematically search for better configurations.\n",
    "\n",
    "The syntax for tuning pipeline parameters is: `stepname__parametername`\n",
    "\n",
    "**Book reference:** See\n",
    "[CSS book Example 11.6](https://cssbook.net/content/chapter11.html#exm-gridsearchlogreg)\n",
    "for a similar grid search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: a small parameter grid\n",
    "param_grid = {\n",
    "    \"vectorizer__ngram_range\": [(1, 1), (1, 2)],  # unigrams vs. unigrams+bigrams\n",
    "    \"vectorizer__min_df\": [1, 5],                  # minimum document frequency\n",
    "    \"classifier__C\": [0.1, 1, 10],                 # regularization strength\n",
    "}\n",
    "\n",
    "# How many combinations is this?\n",
    "n_combinations = 2 * 2 * 3\n",
    "print(f\"Grid has {n_combinations} combinations\")\n",
    "print(f\"With 5-fold CV, that's {n_combinations * 5} model fits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set up and run GridSearchCV\n",
    "# - Use the pipeline (baseline_pipe or create a new one)\n",
    "# - Use param_grid and cv defined above\n",
    "# - Choose an appropriate scoring metric\n",
    "# - Consider setting n_jobs=-1 to use all CPU cores\n",
    "# - Consider setting verbose=1 to see progress\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"vectorizer\", TfidfVectorizer()),\n",
    "    (\"classifier\", LogisticRegression(solver=\"liblinear\", random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "# grid_search = GridSearchCV(\n",
    "#     pipe, \n",
    "#     param_grid, \n",
    "#     cv=cv, \n",
    "#     scoring=\"f1_macro\",\n",
    "#     n_jobs=-1,\n",
    "#     verbose=1\n",
    "# )\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Inspect the results\n",
    "# - What were the best parameters?\n",
    "# - What was the best CV score?\n",
    "\n",
    "# print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "# print(f\"Best CV score: {grid_search.best_score_:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Look at all results as a DataFrame\n",
    "# results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "# results_df[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']].sort_values('rank_test_score').head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Grid Search vs. Random Search\n",
    "\n",
    "As mentioned in the lecture:\n",
    "\n",
    "- **Grid search**: tries all combinations → good for small grids\n",
    "- **Random search**: samples random combinations → better for large search\n",
    "  spaces\n",
    "\n",
    "Random search is often more efficient because some hyperparameters matter more\n",
    "than others. With the same computational budget, random search explores more\n",
    "values of the important parameters.\n",
    "\n",
    "**Rule of thumb:** Small grid (< 50 combinations) → Grid search. Larger → Random\n",
    "search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: RandomizedSearchCV with continuous distributions\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "# For random search, you can specify distributions instead of lists\n",
    "param_distributions = {\n",
    "    \"vectorizer__ngram_range\": [(1, 1), (1, 2), (1, 3)],\n",
    "    \"vectorizer__min_df\": [1, 2, 3, 5, 10],\n",
    "    \"vectorizer__max_df\": [0.5, 0.7, 0.9, 1.0],\n",
    "    \"classifier__C\": loguniform(0.01, 100),  # samples from log-uniform distribution\n",
    "    \"classifier__penalty\": [\"l1\", \"l2\"],\n",
    "}\n",
    "\n",
    "# This would be 3 * 5 * 4 * continuous * 2 = many combinations!\n",
    "# Random search samples n_iter combinations instead of trying all\n",
    "\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     pipe,\n",
    "#     param_distributions,\n",
    "#     n_iter=50,  # try 50 random combinations\n",
    "#     cv=cv,\n",
    "#     scoring=\"f1_macro\",\n",
    "#     random_state=RANDOM_STATE,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "# random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Comparing Multiple Classifiers (Your Turn)\n",
    "\n",
    "The grid above only tested one classifier type. Now extend it to compare\n",
    "different approaches.\n",
    "\n",
    "**Task:** Create a comparison that includes:\n",
    "\n",
    "- At least two different classifiers (e.g., LogisticRegression vs. MultinomialNB\n",
    "  vs. LinearSVC)\n",
    "- Different vectorizer settings\n",
    "- Appropriate parameters for each classifier\n",
    "\n",
    "**Hint:** You can have multiple param grids (as a list) to handle\n",
    "classifier-specific parameters.\n",
    "\n",
    "**Book reference:** See\n",
    "[CSS book Example 11.4](https://cssbook.net/content/chapter11.html#exm-basiccomparisons)\n",
    "for comparing multiple configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of multiple grids for different classifiers:\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"vectorizer\", TfidfVectorizer()),\n",
    "    (\"classifier\", LogisticRegression())  # placeholder, will be replaced by grid\n",
    "])\n",
    "\n",
    "# When you have a list of grids, GridSearchCV tries each one\n",
    "param_grids = [\n",
    "    # Grid 1: Logistic Regression with L2\n",
    "    {\n",
    "        \"vectorizer__ngram_range\": [(1, 1), (1, 2)],\n",
    "        \"vectorizer__min_df\": [1, 5],\n",
    "        \"classifier\": [LogisticRegression(solver=\"liblinear\", random_state=RANDOM_STATE)],\n",
    "        \"classifier__C\": [0.1, 1, 10],\n",
    "        \"classifier__penalty\": [\"l2\"],\n",
    "    },\n",
    "    # Grid 2: Logistic Regression with L1 (for comparison)\n",
    "    {\n",
    "        \"vectorizer__ngram_range\": [(1, 1), (1, 2)],\n",
    "        \"vectorizer__min_df\": [1, 5],\n",
    "        \"classifier\": [LogisticRegression(solver=\"liblinear\", random_state=RANDOM_STATE)],\n",
    "        \"classifier__C\": [0.1, 1, 10],\n",
    "        \"classifier__penalty\": [\"l1\"],\n",
    "    },\n",
    "    # Grid 3: Naive Bayes (no C parameter!)\n",
    "    {\n",
    "        \"vectorizer__ngram_range\": [(1, 1), (1, 2)],\n",
    "        \"vectorizer__min_df\": [1, 5],\n",
    "        \"classifier\": [MultinomialNB()],\n",
    "        \"classifier__alpha\": [0.1, 0.5, 1.0],  # smoothing parameter\n",
    "    },\n",
    "    # TODO: Add Grid 4 for LinearSVC\n",
    "    # Hint: LinearSVC uses C for regularization, similar to LogisticRegression\n",
    "]\n",
    "\n",
    "# YOUR CODE HERE: Run this expanded grid search\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Final Evaluation on Test Set\n",
    "\n",
    "**Only now** do we touch the test set. We use the best model from our grid\n",
    "search and evaluate it once.\n",
    "\n",
    "This gives us an unbiased estimate of how our model will perform on new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best estimator is already fitted on the full training data\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the held-out test set\n",
    "y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate a full classification report\n",
    "# Look at precision, recall, F1 for EACH class\n",
    "\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and display a confusion matrix\n",
    "# Which classes get confused with each other?\n",
    "\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)\n",
    "# disp.plot(cmap='Blues')\n",
    "# plt.title(\"Confusion Matrix on Test Set\")\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Error Analysis\n",
    "\n",
    "Numbers only tell part of the story. Looking at actual misclassifications often\n",
    "reveals systematic issues.\n",
    "\n",
    "**Task:** Examine some misclassified examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with predictions for analysis\n",
    "test_results = pd.DataFrame({\n",
    "    'text': X_test.values,\n",
    "    'true_label': y_test.values,\n",
    "    'predicted': y_pred,\n",
    "    'correct': y_test.values == y_pred\n",
    "})\n",
    "\n",
    "# Look at some misclassifications\n",
    "misclassified = test_results[~test_results['correct']]\n",
    "print(f\"Total misclassifications: {len(misclassified)} out of {len(test_results)} ({100*len(misclassified)/len(test_results):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Examine some misclassified examples\n",
    "# - Are there patterns?\n",
    "# - Are some errors understandable (ambiguous cases)?\n",
    "# - Are some errors surprising (clear cases the model got wrong)?\n",
    "\n",
    "# Random sample of errors:\n",
    "# misclassified.sample(10)\n",
    "\n",
    "# Or filter by specific confusion:\n",
    "# misclassified[(misclassified['true_label'] == 'A') & (misclassified['predicted'] == 'B')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Saving and Loading Your Model\n",
    "\n",
    "Once you have a good model, you'll want to save it so you can:\n",
    "\n",
    "1. Apply it to new data (`unlabeled.csv`) without retraining\n",
    "2. Share it with others\n",
    "3. Use it in production\n",
    "\n",
    "**Important:** You save the entire pipeline (vectorizer + classifier), not just\n",
    "the classifier!\n",
    "\n",
    "**Book reference:** See\n",
    "[CSS book Example 11.8](https://cssbook.net/content/chapter11.html#exm-reuse)\n",
    "for saving and loading models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "joblib.dump(best_model, \"best_text_classifier.joblib\")\n",
    "print(\"Model saved to best_text_classifier.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later: load and use the model\n",
    "loaded_model = joblib.load(\"best_text_classifier.joblib\")\n",
    "\n",
    "# Test that it works\n",
    "sample_texts = [\"This is a test document.\", \"Another example text here.\"]\n",
    "predictions = loaded_model.predict(sample_texts)\n",
    "print(f\"Predictions: {predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply your model to the unlabeled data\n",
    "\n",
    "# unlabeled = pd.read_csv(\"unlabeled.csv\")\n",
    "# unlabeled['predicted_label'] = loaded_model.predict(unlabeled['text'])\n",
    "# unlabeled.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Reflection Questions\n",
    "\n",
    "Before you finish, consider these questions:\n",
    "\n",
    "1. **Is the test performance notably worse than CV performance?** \n",
    "   - If so, why might that be? (Hint: overfitting to validation during tuning?)\n",
    "\n",
    "2. **Are there classes that perform particularly poorly?** \n",
    "   - What could you do about it? (More data? Different features? Class weights?)\n",
    "\n",
    "3. **If you were to write up these results for a thesis**, what would you report? \n",
    "   - Best model and its hyperparameters\n",
    "   - CV performance (mean ± std)\n",
    "   - Test set performance (precision, recall, F1 per class)\n",
    "   - Confusion matrix\n",
    "   - Error analysis insights\n",
    "\n",
    "4. **What caveats would you mention?**\n",
    "   - How well will this generalize to data from different sources/time periods?\n",
    "   - What are the failure modes?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Bonus Challenges (if time permits)\n",
    "\n",
    "### A. Threshold Tuning (Binary Classification)\n",
    "\n",
    "For binary classification: instead of using the default 0.5 threshold, find the\n",
    "optimal threshold using the ROC curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for binary classification!\n",
    "# from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Get predicted probabilities (not just class labels)\n",
    "# Note: LinearSVC doesn't have predict_proba, use LogisticRegression\n",
    "# y_proba = best_model.predict_proba(X_test)[:, 1]  # probability of positive class\n",
    "\n",
    "# fpr, tpr, thresholds = roc_curve(y_test, y_proba, pos_label='pos')  # adjust pos_label\n",
    "# auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc:.3f})')\n",
    "# plt.plot([0, 1], [0, 1], 'k--', label='Random classifier')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC Curve')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Feature Importance\n",
    "\n",
    "What words are most predictive for each class?\n",
    "\n",
    "**Book reference:** See\n",
    "[CSS book Example 11.9](https://cssbook.net/content/chapter11.html#exm-eli5) for\n",
    "using eli5 to inspect features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For logistic regression, you can examine coefficients\n",
    "# (Only works if best_model uses LogisticRegression)\n",
    "\n",
    "# vectorizer = best_model.named_steps['vectorizer']\n",
    "# classifier = best_model.named_steps['classifier']\n",
    "\n",
    "# feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# For binary classification:\n",
    "# coefs = classifier.coef_[0]\n",
    "# feature_importance = pd.DataFrame({\n",
    "#     'feature': feature_names,\n",
    "#     'coefficient': coefs\n",
    "# }).sort_values('coefficient', key=abs, ascending=False)\n",
    "\n",
    "# print(\"Top 20 most predictive features:\")\n",
    "# print(feature_importance.head(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Handling Class Imbalance\n",
    "\n",
    "If you have imbalanced classes, try `class_weight='balanced'` and see if it\n",
    "helps the minority class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with and without class_weight='balanced'\n",
    "\n",
    "# pipe_balanced = Pipeline([\n",
    "#     (\"vectorizer\", TfidfVectorizer(min_df=5, max_df=0.5)),\n",
    "#     (\"classifier\", LogisticRegression(solver=\"liblinear\", class_weight=\"balanced\", random_state=RANDOM_STATE))\n",
    "# ])\n",
    "\n",
    "# scores_balanced = cross_val_score(pipe_balanced, X_train, y_train, cv=cv, scoring=\"f1_macro\")\n",
    "# print(f\"With class_weight='balanced': {scores_balanced.mean():.3f} (+/- {scores_balanced.std():.3f})\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
