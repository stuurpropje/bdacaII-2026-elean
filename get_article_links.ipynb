{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the API endpoint and polite defaults\n",
    "API_URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "USER_AGENT = \"MediaWikiAPITutorial/1.0 (contact: your.email@example.com)\"\n",
    "REQUEST_DELAY = 0.5  # seconds between requests\n",
    "TIMEOUT = 30\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\"User-Agent\": USER_AGENT})\n",
    "\n",
    "def api_get(params: dict) -> dict | None:\n",
    "    \"\"\"Make a safe request to the MediaWiki API with retries and rate limiting.\"\"\"\n",
    "    try:\n",
    "        response = session.get(API_URL, params=params, timeout=TIMEOUT)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request error: {e}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Response was not valid JSON\")\n",
    "        return None\n",
    "    finally:\n",
    "        time.sleep(REQUEST_DELAY)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles linked from the body of 'Fake news' (HTML method):\n",
      "\n",
      "1. Frederick Burr Opper\n",
      "   https://en.wikipedia.org/wiki/Frederick_Burr_Opper\n",
      "\n",
      "2. Misinformation\n",
      "   https://en.wikipedia.org/wiki/Misinformation\n",
      "\n",
      "3. Disinformation\n",
      "   https://en.wikipedia.org/wiki/Disinformation\n",
      "\n",
      "4. Propaganda\n",
      "   https://en.wikipedia.org/wiki/Propaganda\n",
      "\n",
      "5. Hoaxes\n",
      "   https://en.wikipedia.org/wiki/Hoaxes\n",
      "\n",
      "6. News\n",
      "   https://en.wikipedia.org/wiki/News\n",
      "\n",
      "7. Defamation\n",
      "   https://en.wikipedia.org/wiki/Defamation\n",
      "\n",
      "8. Advertising\n",
      "   https://en.wikipedia.org/wiki/Advertising\n",
      "\n",
      "9. Foreign electoral intervention\n",
      "   https://en.wikipedia.org/wiki/Foreign_electoral_intervention\n",
      "\n",
      "10. Satire\n",
      "   https://en.wikipedia.org/wiki/Satire\n",
      "\n",
      "11. Sensationalist\n",
      "   https://en.wikipedia.org/wiki/Sensationalist\n",
      "\n",
      "12. Clickbait\n",
      "   https://en.wikipedia.org/wiki/Clickbait\n",
      "\n",
      "13. Headline\n",
      "   https://en.wikipedia.org/wiki/Headline\n",
      "\n",
      "14. Fake news websites\n",
      "   https://en.wikipedia.org/wiki/Fake_news_websites\n",
      "\n",
      "15. Social media\n",
      "   https://en.wikipedia.org/wiki/Social_media\n",
      "\n",
      "16. Facebook News Feed\n",
      "   https://en.wikipedia.org/wiki/Facebook_News_Feed\n",
      "\n",
      "17. Political polarization\n",
      "   https://en.wikipedia.org/wiki/Political_polarization\n",
      "\n",
      "18. Post-truth politics\n",
      "   https://en.wikipedia.org/wiki/Post-truth_politics\n",
      "\n",
      "19. Motivated reasoning\n",
      "   https://en.wikipedia.org/wiki/Motivated_reasoning\n",
      "\n",
      "20. Confirmation bias\n",
      "   https://en.wikipedia.org/wiki/Confirmation_bias\n",
      "\n",
      "\n",
      "Total links retrieved: 611\n"
     ]
    }
   ],
   "source": [
    "def get_article_links(title, limit=50):\n",
    "    \"\"\"\n",
    "    Get links from a Wikipedia article that appear in the rendered body,\n",
    "    excluding links from \"See also\" and later sections (References,\n",
    "    External links, Notes, Further reading, Bibliography).\n",
    "\n",
    "    Uses the MediaWiki parse API to fetch the rendered HTML, then\n",
    "    BeautifulSoup to extract <a> tags before the first stop-section\n",
    "    heading. Only main-namespace article links are returned (no\n",
    "    File:, Category:, etc.). Links are deduplicated and returned in\n",
    "    the order they appear on the page.\n",
    "\n",
    "    Note: links inside <ref> citations are NOT included because the\n",
    "    rendered HTML moves them to the References section at the bottom.\n",
    "\n",
    "    Parameters:\n",
    "    title (str): The title of the Wikipedia article (e.g. \"Fake news\")\n",
    "    limit (int): Maximum number of links to return (default 50)\n",
    "\n",
    "    Returns:\n",
    "    list[dict]: A list of dictionaries, each with keys:\n",
    "        - 'title' (str): The linked article's title\n",
    "        - 'url'   (str): Full URL to the linked article\n",
    "        Returns an empty list if the article is not found or the\n",
    "        request fails.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'action': 'parse',\n",
    "        'page': title,\n",
    "        'prop': 'text',\n",
    "        'format': 'json'\n",
    "    }\n",
    "    \n",
    "    data = api_get(params)\n",
    "    if not data or 'parse' not in data:\n",
    "        return []\n",
    "    \n",
    "    html = data['parse']['text']['*']\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Find where \"See also\" or similar sections start\n",
    "    stop_sections = ['See also', 'References', 'External links', 'Notes', 'Further reading', 'Bibliography']\n",
    "    stop_element = None\n",
    "    \n",
    "    for heading in soup.find_all('h2'):\n",
    "        # New MediaWiki HTML: <h2 id=\"See_also\">See also</h2>\n",
    "        heading_text = heading.get_text(strip=True)\n",
    "        # Strip any \"[edit]\" suffix that MediaWiki may include\n",
    "        heading_text = heading_text.replace('[edit]', '').strip()\n",
    "        if heading_text in stop_sections:\n",
    "            # The h2 may be wrapped in a <div class=\"mw-heading\">, remove from there\n",
    "            parent = heading.parent\n",
    "            if parent and parent.has_attr('class') and 'mw-heading' in parent.get('class', []):\n",
    "                stop_element = parent\n",
    "            else:\n",
    "                stop_element = heading\n",
    "            break\n",
    "    \n",
    "    # If we found a stop point, remove everything after it\n",
    "    if stop_element:\n",
    "        for sibling in list(stop_element.next_siblings):\n",
    "            sibling.extract()\n",
    "        stop_element.extract()\n",
    "    \n",
    "    # Now extract links from the remaining content\n",
    "    links = []\n",
    "    seen = set()\n",
    "    \n",
    "    for a in soup.find_all('a', href=True):\n",
    "        href = a['href']\n",
    "        # Only include links to main namespace articles\n",
    "        if href.startswith('/wiki/') and ':' not in href.split('/wiki/')[1]:\n",
    "            link_title = href.split('/wiki/')[1].replace('_', ' ')\n",
    "            link_title = requests.utils.unquote(link_title)\n",
    "            \n",
    "            if link_title not in seen:\n",
    "                seen.add(link_title)\n",
    "                links.append({\n",
    "                    'title': link_title,\n",
    "                    'url': f'https://en.wikipedia.org{href}'\n",
    "                })\n",
    "                if len(links) >= limit:\n",
    "                    break\n",
    "    \n",
    "    return links\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "links = get_article_links(\"Fake news\", limit=1000)\n",
    "print(\"Articles linked from the body of 'Fake news' (HTML method):\\n\")\n",
    "for i, link in enumerate(links[:20], 1):\n",
    "    print(f\"{i}. {link['title']}\")\n",
    "    print(f\"   {link['url']}\\n\")\n",
    "print(f\"\\nTotal links retrieved: {len(links)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles linked from the body of 'Fake news' (wikitext method):\n",
      "\n",
      "1. Frederick Burr Opper\n",
      "   https://en.wikipedia.org/wiki/Frederick_Burr_Opper\n",
      "\n",
      "2. misinformation\n",
      "   https://en.wikipedia.org/wiki/misinformation\n",
      "\n",
      "3. disinformation\n",
      "   https://en.wikipedia.org/wiki/disinformation\n",
      "\n",
      "4. propaganda\n",
      "   https://en.wikipedia.org/wiki/propaganda\n",
      "\n",
      "5. hoaxes\n",
      "   https://en.wikipedia.org/wiki/hoaxes\n",
      "\n",
      "6. news\n",
      "   https://en.wikipedia.org/wiki/news\n",
      "\n",
      "7. The Guardian\n",
      "   https://en.wikipedia.org/wiki/The_Guardian\n",
      "\n",
      "8. defamation\n",
      "   https://en.wikipedia.org/wiki/defamation\n",
      "\n",
      "9. U.S. News & World Report\n",
      "   https://en.wikipedia.org/wiki/U.S._News_&_World_Report\n",
      "\n",
      "10. advertising\n",
      "   https://en.wikipedia.org/wiki/advertising\n",
      "\n",
      "11. Merriam-Webster\n",
      "   https://en.wikipedia.org/wiki/Merriam-Webster\n",
      "\n",
      "12. Foreign electoral intervention\n",
      "   https://en.wikipedia.org/wiki/Foreign_electoral_intervention\n",
      "\n",
      "13. Satire\n",
      "   https://en.wikipedia.org/wiki/Satire\n",
      "\n",
      "14. sensationalist\n",
      "   https://en.wikipedia.org/wiki/sensationalist\n",
      "\n",
      "15. clickbait\n",
      "   https://en.wikipedia.org/wiki/clickbait\n",
      "\n",
      "16. headline\n",
      "   https://en.wikipedia.org/wiki/headline\n",
      "\n",
      "17. fake news websites\n",
      "   https://en.wikipedia.org/wiki/fake_news_websites\n",
      "\n",
      "18. social media\n",
      "   https://en.wikipedia.org/wiki/social_media\n",
      "\n",
      "19. Facebook News Feed\n",
      "   https://en.wikipedia.org/wiki/Facebook_News_Feed\n",
      "\n",
      "20. Annals of the International Communication Association\n",
      "   https://en.wikipedia.org/wiki/Annals_of_the_International_Communication_Association\n",
      "\n",
      "\n",
      "Total links retrieved: 699\n"
     ]
    }
   ],
   "source": [
    "def get_article_links_wikitext(title, limit=50):\n",
    "    \"\"\"\n",
    "    Get links from a Wikipedia article before the 'See also' section\n",
    "    by parsing the raw wikitext markup instead of HTML.\n",
    "\n",
    "    Fetches the article's source wikitext via the MediaWiki API, cuts\n",
    "    it off at the \"== See also ==\" heading, and extracts [[wikilink]]\n",
    "    targets using a regular expression. Only main-namespace links are\n",
    "    returned (namespace prefixes like File: or Category: are skipped).\n",
    "    Links are deduplicated and returned in the order they appear in\n",
    "    the source.\n",
    "\n",
    "    Note: unlike the HTML/BeautifulSoup approach, this method DOES\n",
    "    include links inside <ref> citation tags, because those appear\n",
    "    inline in the wikitext source before \"See also\".\n",
    "\n",
    "    Parameters:\n",
    "    title (str): The title of the Wikipedia article (e.g. \"Fake news\")\n",
    "    limit (int): Maximum number of links to return (default 50)\n",
    "\n",
    "    Returns:\n",
    "    list[dict]: A list of dictionaries, each with keys:\n",
    "        - 'title' (str): The linked article's title\n",
    "        - 'url'   (str): Full URL to the linked article\n",
    "        Returns an empty list if the article is not found or the\n",
    "        request fails.\n",
    "    \"\"\"\n",
    "    # Fetch raw wikitext\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'titles': title,\n",
    "        'prop': 'revisions',\n",
    "        'rvprop': 'content',\n",
    "        'rvslots': 'main',\n",
    "    }\n",
    "\n",
    "    data = api_get(params)\n",
    "    if not data or 'query' not in data:\n",
    "        return []\n",
    "\n",
    "    pages = data['query']['pages']\n",
    "    page_id = list(pages.keys())[0]\n",
    "    if page_id == '-1':\n",
    "        return []\n",
    "\n",
    "    wikitext = pages[page_id]['revisions'][0]['slots']['main']['*']\n",
    "\n",
    "    # Cut off everything from \"== See also ==\" onward\n",
    "    parts = re.split(r'==\\s*See also\\s*==', wikitext, maxsplit=1)\n",
    "    body = parts[0]\n",
    "\n",
    "    # Extract [[Link]] and [[Link|display text]] targets\n",
    "    raw_links = re.findall(r'\\[\\[([^#\\]|]+)', body)\n",
    "\n",
    "    # Deduplicate while preserving order, skip non-article namespaces\n",
    "    seen = set()\n",
    "    links = []\n",
    "    for link in raw_links:\n",
    "        name = link.strip()\n",
    "        # Skip namespace links like \"File:\", \"Category:\", etc.\n",
    "        if not name or ':' in name:\n",
    "            continue\n",
    "        if name not in seen:\n",
    "            seen.add(name)\n",
    "            links.append({\n",
    "                'title': name,\n",
    "                'url': f\"https://en.wikipedia.org/wiki/{name.replace(' ', '_')}\"\n",
    "            })\n",
    "            if len(links) >= limit:\n",
    "                break\n",
    "\n",
    "    return links\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "links_wt = get_article_links_wikitext(\"Fake news\", limit=1000)\n",
    "print(\"Articles linked from the body of 'Fake news' (wikitext method):\\n\")\n",
    "for i, link in enumerate(links_wt[:20], 1):\n",
    "    print(f\"{i}. {link['title']}\")\n",
    "    print(f\"   {link['url']}\\n\")\n",
    "print(f\"\\nTotal links retrieved: {len(links_wt)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
